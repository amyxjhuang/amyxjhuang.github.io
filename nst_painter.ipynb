{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNT8Gc30Hi0oxxZnELd0H/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amyxjhuang/amyxjhuang.github.io/blob/master/nst_painter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 19-layer VGG Network, to extract style from the image\n",
        "\n",
        "Sources:\n",
        "* https://www.kaggle.com/code/just4jcgeorge/styletransfer-using-vgg19-pytorch"
      ],
      "metadata": {
        "id": "ch7gF4M75A8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n"
      ],
      "metadata": {
        "id": "Dz9osrPkQ1oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# image = Image.open(\"your_image.jpg\")  # Load image\n",
        "folder_path = \"/content/drive/My Drive/NST Painter/images/\"\n",
        "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n"
      ],
      "metadata": {
        "id": "VyNOp54t_Buw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Found images:\", image_files)"
      ],
      "metadata": {
        "id": "SJ1sHVwlDoO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std = [0.229, 0.224, 0.225]\n",
        "mean = [0.485, 0.456, 0.406]"
      ],
      "metadata": {
        "id": "Jxo2TwpLIRub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-eofFGvYIRh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Define the transformation (resizing, converting to tensor, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),  # Resize to 512 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load all images into a list of tensors\n",
        "image_tensors = [transform(Image.open(img).convert(\"RGB\")).unsqueeze(0) for img in image_files]\n",
        "\n",
        "print(f\"Loaded {len(image_tensors)} images as tensors.\")"
      ],
      "metadata": {
        "id": "CM4GOiLNCR4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(tensor):\n",
        "    image = tensor.cpu().clone().detach().squeeze(0)  # Remove batch dimension\n",
        "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)  # Unnormalize\n",
        "    image = image.clamp(0, 1)  # Clamp values between 0 and 1\n",
        "    plt.imshow(image.permute(1, 2, 0))  # Convert from CxHxW to HxWxC\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "o5ObfN8aBNxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def load_image(image_path, max_size=512):\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure it's in RGB format\n",
        "\n",
        "    # Define image transformation pipeline\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((max_size, max_size)),  # Resize to a square\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Match VGG-19 input\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "\n",
        "def im_convert(tensor: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"Convert a PyTorch tensor to a NumPy image.\"\"\"\n",
        "    image = tensor.to(\"cpu\").clone().detach().numpy().squeeze().transpose(1, 2, 0)  # Convert tensor to NumPy array\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))  # Reverse normalization\n",
        "    image = image.clip(0, 1)  # Clip values to the valid range [0, 1]\n",
        "    return image\n",
        "\n",
        "# content_image = load_image('content_image.png')\n",
        "# style_image = load_image('style_image.png')\n"
      ],
      "metadata": {
        "id": "lkerO1hbRJRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tensor in image_tensors:\n",
        "  show_image(tensor)"
      ],
      "metadata": {
        "id": "OT-veqcn7PV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n"
      ],
      "metadata": {
        "id": "XeYgLsMm95o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a GPU is available we will move the vgg model to that device."
      ],
      "metadata": {
        "id": "vhFmCRLq-UXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "vgg.to(device)\n"
      ],
      "metadata": {
        "id": "0yhxbXzp9783"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(tensor):\n",
        "    # Detach tensor from the computational graph in Pytorch\n",
        "    image = tensor.cpu().clone().detach()\n",
        "\n",
        "    # Convert the tensor to Numpy format\n",
        "    # Squeeze function that we invoke on Numpy will get rid of the batch dimension\n",
        "    # of size 1 dimensions\n",
        "    # The squeeze function in general squeezes out all size 1 dimensions\n",
        "\n",
        "    image = image.cpu().numpy().squeeze()\n",
        "\n",
        "    # The tensor representation of our image has channels in the first dimensions\n",
        "    # then height and width of the image. We need to perform a transpose operation\n",
        "    # so that we get the image in the format where the dimensions are first height\n",
        "    # then width and then the number of channels\n",
        "    # This is what matplotlib expects\n",
        "    image = image.transpose(1, 2, 0)\n",
        "\n",
        "    # Multiply pixel values by std deviation and add the mean\n",
        "    # so that we normalize the image.\n",
        "    image *= np.array(std) + np.array(mean)\n",
        "\n",
        "    # Clip all pixel values in the range 0 to 1 and return this image\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "tQ-yYIGhE7XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = image_tensors[0]\n",
        "style_image = image_tensors[0]"
      ],
      "metadata": {
        "id": "ux5H9bzII0Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = tensor_to_image(style_image)\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Style Image')\n",
        "plt.imshow(img)\n",
        "\n",
        "img = tensor_to_image(content_image)\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Content Image')\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "q-vXtRQNH7VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layers of Interest - these are the layers of our deep CNN model\n"
      ],
      "metadata": {
        "id": "3xbZpRRDonQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAYERS_OF_INTEREST = {\n",
        "    # '0': 'conv1_1',\n",
        "    # '5': 'conv2_1',\n",
        "    '10': 'conv3_1',\n",
        "    '19': 'conv4_1',\n",
        "    '21': 'conv4_2',\n",
        "    '28': 'conv5_1'\n",
        "    }\n",
        "\n",
        "def apply_model_and_extract_features(image, model):\n",
        "  \"\"\" Pass the image through the VGG model and store the intermediate activations / features\n",
        "      for the layers of interest\n",
        "\n",
        "  \"\"\"\n",
        "  x = image\n",
        "\n",
        "  features = {}\n",
        "\n",
        "  for name, layer in model._modules.items():\n",
        "      x = layer(x)\n",
        "\n",
        "      if name in LAYERS_OF_INTEREST:\n",
        "          features[LAYERS_OF_INTEREST[name]] = x\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "z-dhRRR6Is-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = content_image.to(device)\n",
        "style_image = style_image.to(device)\n",
        "\n",
        "content_img_features = apply_model_and_extract_features(content_image, vgg)\n",
        "style_img_features   = apply_model_and_extract_features(style_image, vgg)\n"
      ],
      "metadata": {
        "id": "VnwCsf2EIw3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformation(img):\n",
        "    # Convert image into a tensor [C, H, W] format\n",
        "    # Normalize the input image\n",
        "    tasks = tf.Compose([tf.Resize(512),\n",
        "                        tf.ToTensor(),\n",
        "                        tf.Normalize(mean, std)])\n",
        "\n",
        "    img = tasks(img)\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    return img\n",
        "\n"
      ],
      "metadata": {
        "id": "dsLiup5ORGLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PELWBIQdQuen"
      },
      "outputs": [],
      "source": [
        "def calculate_gram_matrix(tensor):\n",
        "    # Gram Matrix can be used to capture the style of an image G = dot(F, F.T)\n",
        "    # Specifically it captures the correlation between different feature channels\n",
        "    # which represents textures and patterns instead of exact pixel values\n",
        "    _, channels, height, width = tensor.size()\n",
        "\n",
        "    tensor = tensor.view(channels, height * width)\n",
        "\n",
        "    gram_matrix = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    gram_matrix = gram_matrix.div(channels * height * width)\n",
        "\n",
        "    return gram_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_features_gram_matrix = {layer: calculate_gram_matrix(style_img_features[layer]) for layer in\n",
        "                                                    style_img_features}\n",
        "\n",
        "style_features_gram_matrix\n"
      ],
      "metadata": {
        "id": "vJtl2ZinR74_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "weights = {\n",
        "    # 'conv1_1': 1.0,\n",
        "    # 'conv2_1': 0.75,\n",
        "    'conv3_1': 0.35,\n",
        "    'conv4_1': 0.25,\n",
        "    'conv5_1': 0.15}\n",
        "target = torch.nn.Parameter(content_image.clone().to(device))\n",
        "\n",
        "optimizer = optim.Adam([target], lr=0.003)\n"
      ],
      "metadata": {
        "id": "mLWiGFIoIl1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "\n",
        "plt.imshow(tensor_to_image(target))\n"
      ],
      "metadata": {
        "id": "aSZnvHAeKekS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision import transforms as tf\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1000000 - too\n",
        "style_weight = 10000000\n",
        "for i in range(1, 1000):\n",
        "\n",
        "    target_features = apply_model_and_extract_features(target, vgg)\n",
        "\n",
        "    content_loss = F.mse_loss (target_features['conv4_2'], content_img_features['conv4_2'])\n",
        "\n",
        "    style_loss = 0\n",
        "    for layer in weights:\n",
        "\n",
        "        target_feature = target_features[layer]\n",
        "\n",
        "        target_gram_matrix = calculate_gram_matrix(target_feature)\n",
        "        style_gram_matrix = style_features_gram_matrix[layer]\n",
        "\n",
        "        layer_loss = F.mse_loss (target_gram_matrix, style_gram_matrix)\n",
        "        layer_loss *= weights[layer]\n",
        "\n",
        "        _, channels, height, width = target_feature.shape\n",
        "\n",
        "        style_loss += layer_loss\n",
        "\n",
        "    total_loss = style_weight * style_loss + content_loss\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print ('Epoch {}:, Style Loss : {:4f}, Content Loss : {:4f}'.format( i, style_loss, content_loss))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "QzdAza4CLMyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the ImageNet mean and std\n",
        "\n",
        "def denormalize(tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    \"\"\"Convert normalized tensor back to [0,1] range for visualization.\"\"\"\n",
        "    tensor = tensor.cpu().clone().detach()  # Avoid modifying the original tensor\n",
        "    tensor = tensor * std + mean  # Reverse normalization\n",
        "    tensor = torch.clamp(tensor, 0, 1)  # Ensure values are in [0,1]\n",
        "    return tensor\n",
        "\n",
        "# Assuming `image_tensor` is a single image (1,3,H,W)\n",
        "image_tensor = image_tensors[0].squeeze(0)  # Remove batch dimension\n",
        "denorm_image = denormalize(image_tensor)\n",
        "\n",
        "# Convert to numpy and plot\n",
        "plt.imshow(denorm_image.permute(1, 2, 0))  # Convert from (C,H,W) to (H,W,C)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0PN1WDTnoMF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "# ax1.imshow(tensor_to_image(content_image))\n",
        "# ax2.imshow(tensor_to_image(target))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.imshow(tensor_to_image(target))\n"
      ],
      "metadata": {
        "id": "rbHvR2OZMSlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "# ax1.imshow(tensor_to_image(denormalize(content_image)))\n",
        "# ax2.imshow(tensor_to_image(denormalize(target)))\n",
        "plt.figure()\n",
        "\n",
        "plt.imshow(tensor_to_image(denormalize(target)))\n"
      ],
      "metadata": {
        "id": "CT-BqqH4oEXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sfBUWLNGZCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_style_on_image(content_image, style_image):\n",
        "    tensor_image = transform(Image.open(img).convert(\"RGB\")).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "noV_0tSWNzAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTT2i69zf5bv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}